# Job Processing System

This system processes job listings from Brave Search results, verifies they are active, scrapes detailed information, and embeds descriptions in a vector store for semantic search.

## Overview

The job processing pipeline consists of five main steps:

1. **Load Jobs**: Read job listings from `job_results.json` (generated by `brave-job-search.py`)
2. **Verify Jobs**: Check if each job listing is still active
3. **Scrape Jobs**: Extract detailed job information using domain-specific adapters
4. **Embed Jobs**: Create vector embeddings of job descriptions
5. **Store**: Save all data to SQLite database and ChromaDB vector store

## Installation

Install the required dependencies:

```bash
pip install -r requirements.txt
```

## Configuration

Create a `.env` file (or use `.env.example` as a template) with the following optional configuration:

```env
# Database Configuration
DB_PATH=job_search.db
CHROMA_PATH=./chroma_db

# Scraping Configuration
SCRAPE_RATE_LIMIT=2.0  # Seconds between requests (default: 2.0)
REQUEST_TIMEOUT=30     # Request timeout in seconds (default: 30)

# Embedding Configuration
EMBEDDING_MODEL=all-MiniLM-L6-v2  # Sentence transformer model
```

## Usage

### Basic Usage

Process jobs from the default `job_results.json` file:

```bash
python job_processor.py
```

### Programmatic Usage

```python
from job_processor import process_jobs_from_json

# Process jobs from a specific JSON file
process_jobs_from_json("my_job_results.json")
```

### Using Individual Components

You can also use the components individually:

```python
from job_processor import JobDatabase, JobVerifier, JobScraper, JobEmbedder

# Initialize components
db = JobDatabase()
verifier = JobVerifier()
scraper = JobScraper()
embedder = JobEmbedder()

# Verify a job
is_active, message = verifier.verify_job("https://example.com/job/123")

# Scrape a job
details = scraper.scrape_job("https://example.com/job/123")

# Embed a job
embedder.embed_job(job_id=1, job_data={
    "title": "Software Engineer",
    "url": "https://example.com/job/123",
    "company": "Example Corp",
    "location": "Remote",
    "job_description": "We are looking for..."
})

# Search for similar jobs
similar_jobs = embedder.search_similar_jobs("Python developer with ML experience", n_results=5)

# Close database connection
db.close()
```

## Database Schema

### Jobs Table

| Column | Type | Description |
|--------|------|-------------|
| id | INTEGER | Primary key |
| url | TEXT | Job listing URL (unique) |
| title | TEXT | Job title |
| snippet | TEXT | Search result snippet |
| status | TEXT | Processing status (pending, verified, inactive, scraped, scrape_failed, embedded, embed_failed) |
| verified_at | TIMESTAMP | When job was verified |
| scraped_at | TIMESTAMP | When job was scraped |
| job_description | TEXT | Full job description |
| requirements | TEXT | Job requirements |
| benefits | TEXT | Job benefits |
| location | TEXT | Job location |
| company | TEXT | Company name |
| salary | TEXT | Salary information |
| created_at | TIMESTAMP | Record creation time |
| updated_at | TIMESTAMP | Last update time |

### Processing Log Table

| Column | Type | Description |
|--------|------|-------------|
| id | INTEGER | Primary key |
| job_id | INTEGER | Foreign key to jobs table |
| action | TEXT | Action performed |
| status | TEXT | Status of action |
| message | TEXT | Additional message |
| timestamp | TIMESTAMP | When action occurred |

## Domain-Specific Scraping Adapters

The system includes adapters for different ATS (Applicant Tracking System) platforms:

### Supported Platforms

1. **Greenhouse** (`boards.greenhouse.io`)
   - Extracts company name from URL
   - Parses job description, requirements, and benefits from structured sections
   - Handles location and salary information

2. **Lever** (`jobs.lever.co`)
   - Extracts company name from URL
   - Parses structured sections for description, requirements, and benefits
   - Handles location and salary information

3. **Workday** (`workday.com`, `myworkdayjobs.com`)
   - Uses data attributes for element identification
   - Extracts job description, qualifications, and location
   - Handles company information

4. **SmartRecruiters** (`smartrecruiters.com`)
   - Uses data-test attributes for element identification
   - Extracts job description, requirements, and location
   - Handles company information

5. **Generic Adapter** (Fallback)
   - Works with any job board
   - Uses heuristic-based extraction
   - Falls back to largest text block for description

### Adding Custom Adapters

To add support for a new ATS platform:

1. Create a new adapter class in `scraping_adapters.py`:

```python
class MyCustomAdapter(BaseScraperAdapter):
    def __init__(self):
        super().__init__()
        self.domain = "myplatform.com"
    
    def can_handle(self, url: str) -> bool:
        return "myplatform.com" in url
    
    def scrape(self, soup: BeautifulSoup, url: str) -> Dict:
        details = {
            "job_description": "",
            "requirements": "",
            "benefits": "",
            "location": "",
            "company": "",
            "salary": ""
        }
        
        # Implement your scraping logic here
        # Use soup to parse HTML and extract information
        
        return details
```

2. Register the adapter in `AdapterRegistry._register_default_adapters()`:

```python
def _register_default_adapters(self):
    self.register_adapter(GreenhouseAdapter())
    self.register_adapter(LeverAdapter())
    self.register_adapter(WorkdayAdapter())
    self.register_adapter(SmartRecruitersAdapter())
    self.register_adapter(MyCustomAdapter())  # Add your adapter
    self.register_adapter(GenericAdapter())  # Always last
```

## Rate Limiting

The system respects rate limiting to avoid overwhelming job boards:

- **Default**: 2 seconds between requests
- **Configurable**: Set `SCRAPE_RATE_LIMIT` in `.env` file
- **Applied to**: Both verification and scraping operations

## Logging

All operations are logged to `job_processor.log` with timestamps and severity levels:

```log
2026-01-23 14:20:00 - INFO - Database initialized successfully
2026-01-23 14:20:01 - INFO - Inserted job: Software Engineer (ID: 1)
2026-01-23 14:20:03 - INFO - Job 1 verified: Software Engineer
2026-01-23 14:20:05 - INFO - Using adapter 'GreenhouseAdapter' for https://boards.greenhouse.io/example/job/123
2026-01-23 14:20:06 - INFO - Successfully scraped job from https://boards.greenhouse.io/example/job/123
2026-01-23 14:20:08 - INFO - Embedded job 1: Software Engineer
```

## Vector Search with ChromaDB

After embedding jobs, you can perform semantic search:

```python
from job_processor import JobEmbedder

embedder = JobEmbedder()

# Search for similar jobs
results = embedder.search_similar_jobs(
    "Remote Python developer with machine learning experience",
    n_results=5
)

for job in results:
    print(f"Title: {job['title']}")
    print(f"Company: {job['company']}")
    print(f"Location: {job['location']}")
    print(f"URL: {job['url']}")
    print(f"Distance: {job['distance']}")
    print("---")
```

## Troubleshooting

### Jobs not being scraped

- Check `job_processor.log` for error messages
- Verify the URL is accessible
- Check if the job board requires JavaScript (this system only handles static HTML)
- Consider adding a custom adapter for the specific platform

### Embedding errors

- Ensure you have enough disk space for the ChromaDB vector store
- Check that the embedding model is downloaded (it will download automatically on first use)
- Verify job descriptions are not empty before embedding

### Rate limiting issues

- Increase `SCRAPE_RATE_LIMIT` in `.env` if you're getting blocked
- Decrease it if you want faster processing (but risk being blocked)
- Monitor logs for HTTP 429 (Too Many Requests) errors

### Database locked errors

- Ensure only one instance of the script is running
- Check that no other process is accessing the database
- Close database connections properly after use

## Performance Considerations

- **Verification**: ~2 seconds per job (with default rate limit)
- **Scraping**: ~2 seconds per job (with default rate limit)
- **Embedding**: ~0.5-1 second per job (depends on model and hardware)
- **Total time**: ~4-5 seconds per job for full pipeline

For large job lists, consider:
- Processing in batches
- Adjusting rate limits based on job board policies
- Using a faster embedding model if needed

## Best Practices

1. **Run after job search**: Always run `brave-job-search.py` first to generate `job_results.json`
2. **Monitor logs**: Check `job_processor.log` for errors and warnings
3. **Respect rate limits**: Don't set `SCRAPE_RATE_LIMIT` too low to avoid being blocked
4. **Regular backups**: Backup `job_search.db` and `chroma_db` directory regularly
5. **Custom adapters**: Add custom adapters for frequently encountered job boards

## Example Workflow

```bash
# Step 1: Search for jobs
python brave-job-search.py

# Step 2: Process the results
python job_processor.py

# Step 3: Query the database (optional)
sqlite3 job_search.db "SELECT title, company, location FROM jobs WHERE status = 'embedded'"

# Step 4: Search for similar jobs (optional)
python -c "
from job_processor import JobEmbedder
embedder = JobEmbedder()
results = embedder.search_similar_jobs('Python developer', n_results=3)
for job in results:
    print(f'{job[\"title\"]} at {job[\"company\"]} - {job[\"location\"]}')
"
```

## License

This system is part of the brave-job-search project. See LICENSE.md for details.